\documentclass[a4paper]{scrartcl}

\usepackage[english]{babel}
\usepackage[a4paper]{geometry}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{hyperref}
\usepackage{url}
\usepackage{proof}

\usepackage{listings}
\lstloadlanguages{Haskell}
\lstnewenvironment{code}
    {\lstset{}%
      \csname lst@SetFirstLabel\endcsname}
    {\csname lst@SaveFirstLabel\endcsname}
    \lstset{
      basicstyle=\small\ttfamily,
      language=haskell,
      flexiblecolumns=false,
      basewidth={0.5em,0.45em},
      literate={+}{{$+$}}1 {/}{{$/$}}1 {*}{{$*$}}1 {=}{{$=$}}1
               {>}{{$>$}}1 {<}{{$<$}}1 {\\}{{$\lambda$}}1
               {\\\\}{{\char`\\\char`\\}}1
               {->}{{$\rightarrow$}}2 {>=}{{$\geq$}}2 {<-}{{$\leftarrow$}}2
               {<=}{{$\leq$}}2 {=>}{{$\Rightarrow$}}2 
               {\ .}{{$\circ$}}2 {\ .\ }{{$\circ$}}2
               {>>}{{>>}}2 {>>=}{{>>=}}2
               {|}{{$\mid$}}1               
    }

\newcommand{\hsterm}[1]{%
    \setlength{\fboxsep}{3pt}%
    \setlength{\fboxrule}{0.2pt}%
    \fbox{\texttt{\small #1}}}
\renewcommand{\vec}[1]{\overline{#1}}

\newcommand{\algow}{algorithm~$\mathcal{W}$}

\DeclareMathOperator{\zip}{zip}
\DeclareMathOperator{\curry}{curry}

\title{Control Flow Analysis of monomorphic Haskell}
\author{Stijn van Drongelen {\tiny(3117537)},
        Tom Tervoort {\tiny(3470784)},
        Gerben van Veenendaal {\tiny(3460692)}}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
The goal of this project is to implement CFA (control flow analysis) on a small
subset of Haskell. We use \texttt{haskell-src}, so we can only deal with
Haskell~98 to begin with, but that's still a language with more features than
we can handle.

\begin{description}
\item[No guards]
    Having to deal with guards makes processing \texttt{case} constructs very
    hard. We choose not to support them, and as a consequence, we also don't
    use them in our examples.
\item[No type classes]
    Type classes would require a lot of extra work in the type inference
    algorithm.
\item[No type signatures]
    Type inference on Haskell 98 is decidable, and we don't have to resolve
    ambiguous types due to our lack of type classes, so it's not really worth
    implementing support for type signatures.
\item[Monomorphic literals]
    Literals (at least the numeric ones) are polymorphic in Haskell 98, with
    a type class restriction. Since we can't use type classes, we make literals
    monomorphic instead.
\item[Monomorphic types]
    Our subset of Haskell is not polymorphic.
\item[No do-notation or list comprehensions]
    These constructs are pure syntactic sugar. We don't use them in our examples,
    so we don't bother to implement a desugaring for them.
\item[No modularity]
    Since our examples each fit in a single module, we don't implement module
    imports or exports (let alone the FFI).
\end{description}

In our case, the CFA serves to determine the creation sites of both functions
and data types. In case of functions, we want to know for each application
which abstractions may be applied, and in case of data types, we want to know
where the scrutinee of a \texttt{case} expression is finally constructed
(i.e. all fields of the constructor have been filled).

\section{The language}
Every module is assumed to define a function \texttt{main}. Under this
assumption, we build a \texttt{Term} which expresses the value of this
function:\footnote{There may be some differences between this listing and the implementation.}

\begin{code}
data Term = LiteralTerm     Literal
          | VariableTerm    Name
          | ConstructorTerm Name
          | HardwiredTerm   HardwiredValue
          | ApplicationTerm Term Term
          | AbstractionTerm Name Term
          | LetInTerm       Name Term Term
          | CaseTerm        Term [(Pattern, Term)]
          | FixTerm         Term

data Literal = IntegralLit Integer
             | FractionalLit Rational
             | StringLit String
             | CharLit Char

data HardwiredValue = HwTupleCon Int 
                    | HwListCons
                    | HwListNil

data Pattern = Variable Name
             | Pattern {ctorName :: Name, ctorArgs :: [Name]}
\end{code}

Note that, even though \texttt{LetInTerm} is much more simplistic than
Haskell's let-bindings, it is sufficient due to the inclusion of a least
fixed point operator (\texttt{FixTerm}), which serves the same purpose as
\texttt{fix} from \texttt{Data.Function}.

Also note the absence of data type definitions. They are supported in our
language, but are parsed and treated separately. Data constructors are
inserted immediately into the environment, the ADT relationship is however
forgotten.

\subsection{Type rules}
The type universe of our language is defined by the following data types:%
\footnote{There may be some differences between this listing and the implementation.}

\begin{code}
data Type = BasicType AnnVar BasicType
          | DataType AnnVar Name
          | ListType AnnVar Type
          | TupleType AnnVar [Type]
          | Arrow AnnVar Bool Type Type
          | TyVar Name

data BasicType = Integer
               | Char
               | Double

type AnnVar = Name
\end{code}

Note the lack of a \texttt{Forall} construct: we don't support polymorphism,
and the \texttt{TyVar}s are only there to aid \algow{}.

An explanation for the \texttt{AnnVar} and \texttt{Bool} fields follows in
section \ref{sec:algow-phase}.

For sake of space, we omit the rules for defining the environment and
type instantiation.

\subsubsection{Type rules for terms}
Variables:
\begin{equation*}
\infer[Var]%
    {\Gamma \vdash \hsterm{VariableTerm \ensuremath{x}} : \tau}
    {\Gamma(x) = \tau}
\end{equation*}

The rules for the hardwired types (literals, lists, and tuples) are axioms
and simple recursive rules:
\begin{equation*}
\infer[LitIntegral]%
    {\Gamma \vdash \hsterm{LiteralTerm (IntegerLit \_)} : \hsterm{BasicType Integer}}%
    {}
\end{equation*}\begin{equation*}
\infer[LitChar]%
    {\Gamma \vdash \hsterm{LiteralTerm (CharLit \_)} : \hsterm{BasicType Char}}%
    {}
\end{equation*}\begin{equation*}
\infer[LitString]%
    {\Gamma \vdash \hsterm{LiteralTerm (StringLit \_)} : \hsterm{ListType (BasicType Char)}}%
    {}
\end{equation*}\begin{equation*}
\infer[LitFractional]%
    {\Gamma \vdash \hsterm{LiteralTerm (FractionalLit \_)} : \hsterm{BasicType Double}}%
    {}
\end{equation*}\begin{equation*}
\infer[TupleConIntro]%
    {\Gamma \vdash \hsterm{HardwiredTerm (HwTupleCon \ensuremath{n})} : \ldots}%
    {}
\end{equation*}\begin{equation*}
\infer[ListConsIntro]%
    {\Gamma \vdash \hsterm{HardwiredTerm HwListCons} : \hsterm{Arrow \ensuremath{a} (Arrow (ListType \ensuremath{a}) (ListType \ensuremath{a}))}}%
    {}
\end{equation*}\begin{equation*}
\infer[ListNilIntro]%
    {\Gamma \vdash \hsterm{HardwiredTerm HwListNil} : \hsterm{ListType \ensuremath{a}}}%
    {}
\end{equation*}

Abstraction, application, and the fixed point combinator:
\begin{equation*}
\infer[FunIntro]%
    {\Gamma \vdash \hsterm{AbstractionTerm \ensuremath{x} \ensuremath{e}} : \hsterm{Arrow \ensuremath{\tau_x} \ensuremath{\tau_e}}}%
    {\Gamma [x : \tau_x] \vdash e : \tau_e}%
\end{equation*}\begin{equation*}
\infer[FunElim]%
    {\Gamma \vdash \hsterm{ApplicationTerm \ensuremath{e_0} \ensuremath{e_1}} : \tau_2}%
    {\Gamma \vdash e_0 : \hsterm{Arrow \ensuremath{\tau_1} \ensuremath{\tau_2}} \qquad \Gamma \vdash e_1 : \tau_1}%
\end{equation*}\begin{equation*}
\infer[FunFix]%
    {\Gamma \vdash \hsterm{FixTerm \ensuremath{e}} : \ensuremath{a}}%
    {\Gamma \vdash \ensuremath{e} : \hsterm{Arrow (Arrow \ensuremath{a} \ensuremath{a}) \ensuremath{a}}}
\end{equation*}

Declarations:
\begin{align*}
\infer[LetIn]%
    {\Gamma \vdash \hsterm{LetInTerm \ensuremath{x} \ensuremath{e_1} \ensuremath{e_0}} : \tau_0}%
    {\Gamma \vdash e_1 : \tau_1 \qquad \Gamma [x : \tau_1] \vdash e_0 : \tau_0}
\end{align*}

Case matching:
\begin{align*}
\infer[DataElim]%
    {\Gamma \vdash \hsterm{CaseTerm \ensuremath{s} \ensuremath{\vec{m}}} : \tau_e}%
    {\Gamma \vdash s : \tau_s
    \qquad
    \forall \hsterm{(\ensuremath{p}, \ensuremath{e})} \in \vec{m}.\left(
        \Gamma \vdash_\textsc{Patt} p : \tau_s \& b \qquad
        \Gamma; b \vdash e : \tau_e
    \right)}
\end{align*}

\subsubsection{Type rules for patterns}
For patterns, we need slightly different judgements. Patterns don't just have
a type, but they also bind variables, for which the type should might be known.
As seen in the \textit{DataElim} rule, pattern type judgements take the form
\begin{equation*}
    \Gamma \vdash_\textsc{Patt} p : \tau \& b,
\end{equation*}
where $p$ is the pattern, $\tau$ is the type of the pattern, and $b$ is
a sequence of mappings from variables (as bound in $p$) to types (as implied
by $p$).

\begin{equation*}
\infer[VarPatt]%
    {\Gamma \vdash_\textsc{Patt} \hsterm{Variable \ensuremath{x}} : \tau \& [x : \tau]}%
    {}
\end{equation*}

\begin{equation*}
\infer[ConPatt]%
    {\Gamma \vdash_\textsc{Patt} \hsterm{Patt \ensuremath{c} \ensuremath{\vec{p}}} : \tau_t \& \bigcup \vec{b}}%
    {\Gamma \vdash c : \tau_c
    \qquad \forall (p,\tau_p,b) \in \zip(\vec{p}, \vec{\tau_p}, \vec{b}).
        \Gamma \vdash_\textsc{Patt} p : \tau_p \& b
    \qquad \text{$\vec{b}$ pairwise disjoint}}
\end{equation*}
where $(\vec{\tau_p}, \tau_t) = \curry \tau_c$.

\section{Implementation}
\subsection{Parsing and desugaring}
After reading the file and parsing it with \texttt{haskell-src}, our
implementation translates the resulting code into our limited AST type.
Part of this phase involves determining the strongly connected components
in the `dependency' graph of the functions, so that we can rewrite mutually
recursive functions in terms of the fixed point combinator.

If there's anything we encounter that we can't support, we throw an error.

\subsection{Name adornment}
We then traverse the desugared program, and `adorn' every node with a location.
We chose to keep things simple (and unfortunately, unclear for more complex
programs, and useless for further automated use): we only record the name that's
given to the expression, and whether it's `shallow' (the expression is immediately
bound to the name) or `deep' (the term is part of the expression being bound
to a name).

These names are used to build the answer to the CFA. As a consequence, the CFA
will only be able to tell, for function application, which name the applied
function has, and otherwise the name of some expression where that function
is defined.

\subsection{Type inference and constraint generation}
\label{sec:algow-phase}
We then apply a very straightforward implementation of \algow{} to the
desugared program. During \algow{}, we also collect constraints on
the annotation variables (the \texttt{AnnVar} fields) in the types we inferred.

Generally speaking, these annotation variables refer to the site of
construction:
\begin{itemize}
\item
    For \texttt{Arrow} (function types), they can only refer to
    an \texttt{AbstractionTerm}, or to a data type declaration.
\item
    For \texttt{BasicType} (containing only certain hardwired types), they can
    only refer to a \texttt{LiteralTerm}.
\item
    For all other types, they refer to a \texttt{ConstructorTerm}
    (for nullary data constructors) or an \texttt{ApplicationTerm}
    (for data constructors with at least one field).
\end{itemize}

In order to emit constraints for data constructors with at least one field,
we need to know whether an \texttt{ApplicationTerm} is applying a constructor
(partially applied or not) in such a way that a value of the resulting data
type is constructed. The \texttt{Bool} field of \texttt{Arrow} exists to
determine whether the function is a constructor or not.

The constraints are generated at the creation sites $s$ mentioned above:
\begin{itemize}
\item
    For \texttt{AbstractionTerm}s, we generate a fresh annotation variable
\item
    For \texttt{LiteralTerm}s and nullary constructors, we do something similar.
\item
    For \texttt{ApplicationTerm}s, we check whether the applied function is
    a constructor and whether we've completed all fields; if so, we add
    a constraint $s \in \phi$ on the resulting data type.
\end{itemize}
Constraints in the form $\phi \simeq \phi'$ are also generated, as a result
of the unification process during \algow{}.

\subsection{Constraint solving}
After \algow{} is done, we solve the constraints on the annotation variables
separately. First, we build a union-find structure based on the
$\phi \simeq \phi'$ constraints. Then, for the $n \in \phi$ constraints,
we build a mapping from the representatives of $\phi$ (as found in
the union-find structure) to sets of related $n$ from the constraints.
We end up with a map from representative annotation variables to minimal sets
of function names, and a way to translate any annotation variable to its
representative.

\subsection{Output}
Finally, we print the following information:
\begin{itemize}
\item The desugared form of the input program.
\item The inferred type of the \texttt{main} function.
\item For every function application and every case expression, the result of
    the analysis.
\end{itemize}

\section{Examples}

To run the analysis on a source file, use \texttt{cabal} to build
the executable \texttt{haskellcf}, and then execute it with the file of
interest as its only argument.

The output is structured according to a depth-first search through the AST,
searching for function applications and case statements.

\subsection{\texttt{examples/ternary.hs}}

This file defines some basic balanced ternary arithmetic functions.

In the analysis results, the first three data analysis results (identified
by the record starting with ``Scrutinee'') are quite telling for how our
analysis works:
\begin{itemize}
\item
    The first record refers to the case statement in \texttt{negate},
    scrutinizing its only argument. According to our analysis, it might have
    been created somewhere inside \texttt{main}, \texttt{add}, \texttt{multiply}
    or \texttt{negate}.

    This is a pretty useless result, caused by the call to \texttt{negate}
    from \texttt{multiply}. Somehow, the functions inside \texttt{multiply}
    are contaminating the constraint pool with useless information, and the
    call to \texttt{negate} in \texttt{multiply} infects this result.
\item
    The second record refers to the case statement in \texttt{multiply},
    scrutinizing its first argument. The analysis says that it might
    be \texttt{main}, which is correct: it can only be \texttt{Neg}.
\item
    The third record refers to the case statement in \texttt{add},
    scrutinizing its first argument. The analysis did not find any
    possible creation sites, because \texttt{add} is dead code, along with
    \texttt{subtract}.
\end{itemize}

Besides the strange one from \texttt{multiply}, the analysis results are
be straigtforward: function $f$ can only be function $f$, and after partial
applicaiton, you are `inside` function $f$.

\subsection{\texttt{examples/church-booleans.hs}}

This file defines Church-encoded booleans.

The results are not entirely remarkable, and due to our monomorpishm,
the truth value of \texttt{main} can't be derived from its underlying type
(it's $a \rightarrow a \rightarrow a$, instead of something useful like
$a \rightarrow b \rightarrow a$, meaning `true', or $b \rightarrow a \rightarrow a$,
meaning `false'). However, also because we have monomorphism, our function type
annotations can show us exactly what the only possible values for sub-expressions
are inside the combinators. The nicest record comes from an application inside
\texttt{and}:

\begin{verbatim}
Left hand side: (lhs ((rhs t) f))
Right hand side: f
Left hand side type: $96 -> $96
Right hand side type: $96
Possible named functions: {inside true}
\end{verbatim}

\section{Discussion and conclusion}
Our analysis of this subset of Haskell seems to perform moderately. The output
of our program is quite cumbersome to read, but it should be doable.

Our implementation has a few warts:
\begin{itemize}
\item
    Our implementation of \algow{} might have been much cleaner
    if we had separated type constraint generation and solving,
    and if we had used an appropriate monadic (or even applicative)
    functor. However, we already had a fairly straightforward
    implementation of \algow{} available.
\item
    The separation between constructors and variables is quite ugly,
    since the distinction wasn't there for a long time. In expressions,
    they were both desugared to the same thing (variables).
\item
    We don't always properly generate fresh variables. We do have
    an interface for fresh names, but at some places, we simply use
    names that are forbidden in Haskell.
\item
    Type-wise, the code is messy. There is very little distinction
    between names in terms and types, and no distinction between
    annotations on functions and annotations on data types.
\end{itemize}

There are also some missing features. 
\begin{itemize}
\item
    For instance, user-defined constructors do not have annotations on the entire
    `spine' of their function type, and as such, after partial application, they
    do not show up in the CFA result. This in contrast to the family of tuple
    constructors, where for instance the function \texttt{(,,,) x} is known to be
    an `inside' function of the constructor.
\item
    The results of the analysis can be quite redundant, mentioning that a named
    function might be itself. Look for instance at \texttt{examples/useless.hs},
    where the only result is that ``\texttt{id} is possibly the named function
    \texttt{id}''.
\end{itemize}

To support polymorphism, we would have to change a few things about our analysis,
besides the obvious lack of universally quantified types. For one, we don't
think the \texttt{Bool} field of \texttt{Arrow} (a hack to distinguish
constructors from functions) will still work in a polymorphic setting.%
\footnote{We are not even sure if it makes sense in our monomorphic language.}
We would need an additional constraint between function annotations and
data annotations; let's call it $[s \in \delta]_\phi$. We eject this constraint
for sites $s$, where a function $f : \tau_1 \xrightarrow{\phi} \tau_2^\delta$
is applied to $x : \tau_1$. This constraint means, ``if $\phi$ might be
a `final constructor'%
\footnote{A constructor that's partially applied to the point that it's only
missing its last field.}
then $s \in \delta$ must hold.'' Extending the solver to support this should be
trivial, but with more precise semantics than we currently support ($\phi$ must
be a final constructor for $\delta$), this would become trickier.

\end{document}
